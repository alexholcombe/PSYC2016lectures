{
  "hash": "fe74427782928387abd36094caedf483",
  "result": {
    "markdown": "---\ntitle: \"PSYC3888 Can we trust this science?\"\ntitle-slide-attributes:\n    data-background-image: \"images/2targets3objectsPerArray.gif\"\n    data-background-size: contain\n    data-background-opacity: \".1\"\nformat:\n  revealjs:\n    theme: [default, mystyle.scss]\n    incremental: true\n    center: false\n    controls: true  \n---\n\n\n## Can we trust this science? {background-color=\"black\"}\n\n#### Alex Holcombe\n\n#### University of Sydney\n\n<!-- For github deployment, followed these instructions for github pages https://quarto.org/docs/publishing/github-pages.html -->\n\n\n\n<!-- When I wrote this book one of the things I noticed was that to explain MOT and its role in the mind one mainly ended up explaining things about the mind that we know from other paradigms.\n\nNot every field of psychology is like that.\nThis is not a unified theory of tracking. That would be a different talk. This is broad empirical generalizations, the ingredients to a unified theory.\n-->\n\n## Five lessons from studies of multiple object tracking\n\n::: {.nonincremental}\n\n1. Feature binding requires focused attention\n2. Where, but not what!\n3. Split-brain selection\n4. Unitary cognition contaminates many studies\n5. Spatial and temporal crowding constrains selection\n\n:::\n\n::: notes\n\nIn writing this little book, I distilled the results of hundreds of studies to a short list of empirical generalizations. Broad lessons for how cognition and perception connect, it turns out.\n\nIn some ways, visual attention and memory are a coherent scientific field. I mean that findings from paradigms other than MOT generalize to MOT too.\n\n:::\n\n## 1.  \\ \\ \\  Feature binding requires focused attention\n\n![](images/inspectorCartoon.png)\n\n## 1.  \\ \\ \\  Feature binding requires focused attention\n\n<BR>\n\n### Color attention is sufficient\n\nTargets: [1]{style=\"color: green;\"}[1]{style=\"color: blue;\"}[1]{style=\"color: brown;\"}\n\nDistractors: [1]{style=\"color: red;\"}[1]{style=\"color: pink;\"}[1]{style=\"color: blue;\"}\n\n### Shape attention is sufficient\n\nTargets: [1]{style=\"color: green;\"}[2]{style=\"color: green;\"}[3]{style=\"color: green;\"}\n\nDistractors: [4]{style=\"color: green;\"}[5]{style=\"color: green;\"}[6]{style=\"color: green;\"}\n\n## 1.  \\ \\ \\  Feature binding requires focused attention\n\n<BR>\n\n### Color attention is sufficient\n\nTargets: [1]{style=\"color: green;\"}[1]{style=\"color: blue;\"}[1]{style=\"color: brown;\"}\n\nDistractors: [1]{style=\"color: red;\"}[1]{style=\"color: pink;\"}[1]{style=\"color: blue;\"}\n\n### Shape attention is sufficient\n\nTargets: [1]{style=\"color: green;\"}[2]{style=\"color: green;\"}[3]{style=\"color: green;\"}\n\nDistractors: [4]{style=\"color: green;\"}[5]{style=\"color: green;\"}[6]{style=\"color: green;\"}\n\n### Color-shape? No!\n\nTargets: [1]{style=\"color: green;\"}[2]{style=\"color: blue;\"}[3]{style=\"color: brown;\"}\n\nDistractors: [2]{style=\"color: red;\"}[3]{style=\"color: blue;\"}[1]{style=\"color: brown;\"}\n\n::: notes\nWhat you learn in every cognitive psychology class\n:::\n\n<!--Escape space with slashes-->\n## 1. \\ \\ \\ Feature binding requires focused attention\n\n![](images/inspectorCartoon.png)\n\n> Did the enhancement from distinct identities extend to a condition where the objects were distinct in terms of *a combination of colour and digit, but not in colour or digit alone*? Our results clearly showed that it *did not*.\n\nMakovski & Jiang (2009)\n\n::: notes\n\n:::\n\n## Can't judge spatial relations if you can't select {.smaller}\n\n![](images/HolcombeLinaresVaziriPashkam.png)\n::: notes\n\nSelection is not just about tracking.\n\nKnock-on consequences, for judging basic things about the scene\n\n:::\n\nHolcombe, Linares, Vaziri-Pashkam (2011)\n\n## 2.  \\ \\ \\ Where, but not what!\n\n<BR><BR>\n\n![](images/puzzling.png)\n\n## 2.  \\ \\ \\ Where, but not what!\n\n<BR>\n\n![](images/whereButNotWhat.png){.r-stretch}\n\n- Features are updated poorly as an object moves, or not at all (e.g. Horowitz et al., 2007)\n\n- VSTM remains stuck in the past\n\n\n::: notes\nHorowitz et al. (2007), who had participants track targets with unique appearances - the stimuli were cartoon animals in one set of experiments. At the end of each trial, the targets moved behind occluders so that their identities were no longer visible. Participants were asked where a particular target (say, the cartoon rabbit) had gone - that is, which occluder it was hiding behind. This type of task had been dubbed \"multiple identity tracking\" by Oksama and Hyönä (2004). Performance was better than chance, but was worse than the standard MOT task of reporting target locations irrespective of which target a location belonged to.\n\nNot fully anticipated from other visual attention paradigms. Somewhat suggested by FIT.\n\nDistinct process Yet to be integrated with VSTM theories. <!--Like it came up with Brad Wyble's recent theory-->\n:::\n\n## 3.  \\ \\ \\ Split-brain selection\n\n![](images/splitbrain.png)\n\n\n```{=html}\n<!--\n- MOT may isolate location selection\n- \"spatial selection appears to occur at a hemifield-specific stage, with other features subsequently updated and linked in at a field-wide stage.\"-->\n```\n\n## 3.  \\ \\ \\ Split-brain selection\n\n![](images/splitbrainWithPools.png)\n\n- A remarkable ~90% independence\n- Semi-independent limits for location selection and updating\n\n::: notes\n\nExciting\n\n:::\n\n## 2 and 3. \\ \\ Where is split-brain, what is full-brain\n\n![](images/splitbrainWithPools.png)\n\n![](images/whereButNotWhat.png)\n\nSpatial selection appears to occur at a hemifield-specific stage, with other features subsequently updated and linked in at a field-wide stage.\n\n##\n\n::: notes\nA lot of people are studying the top or the bottom. Almost nobody's studying the middle We didn't have a way to get at it before.\n:::\n\n<!--Potentially add an object creation process-->\n\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"grViz html-widget html-fill-item-overflow-hidden html-fill-item\" id=\"htmlwidget-18c79a5d6f8f1b7b376f\" style=\"width:960px;height:480px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-18c79a5d6f8f1b7b376f\">{\"x\":{\"diagram\":\"digraph {\\n\\ngraph [layout = dot, rankdir = BT]\\n\\n# define the global styles of the nodes. We can override these in box if we wish\\nnode [shape = rectangle, style = filled, fillcolor = Linen]\\n\\n#Define the nodes\\nL [label = \\\"LH retinotopy\\\", width=5, height=1, fillcolor = firebrick]\\nR [label = \\\"RH retinotopy\\\", width=5, height=1, fillcolor = firebrick]\\nselectnLH [label =  \\\"parietal selection\\\"]\\nselectnRH [label =  \\\"parietal selection\\\"]\\noutput [label = \\\"Visual Working Memory\\\", width=6, height=2, color=White, fillcolor=gold1]\\n\\n\\n# edge definitions with the node IDs\\nedge [label=\\\"  bottleneck\\\", penwidth=.3]\\nL  -> selectnLH;\\nedge [label=\\\"\\\", penwidth=3]\\nselectnLH -> output\\n\\nedge [label=\\\"  bottleneck\\\", penwidth=.3]\\nR -> selectnRH\\nedge [label=\\\"\\\", penwidth=3]\\nselectnRH -> output\\n\\n\\n}\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n:::\n:::\n\n\n## 4.  \\ \\ \\ Unitary cognition contaminates many studies\n\n![](images/attendToPartsOfSoccerPlayers.png)\n\n## 4.  \\ \\ \\ Unitary cognition contaminates many studies\n\n![](images/trackingCognitiveSpotlight.png)\n- Always test for hemifield independence!\nIf the effect of the factor is twice as big when you distribute targets across hemifields, it's not cognitive contamination.\n\n::: notes\nWith tracking you can actually isolate a hemisphere-specific process.\n\nI read a lot of papers that say that this factor or that factor influence MOT performance, and they imply that they've revealed something about how the tracking process works, but actually it could just be cognition following a single target and boosting performance.\n\nA role for \n:::\n\n## 5.  \\ \\ \\ Crowding constrains selection: Spatial\n\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"grViz html-widget html-fill-item-overflow-hidden html-fill-item\" id=\"htmlwidget-37dadf0d192d376510a1\" style=\"width:960px;height:480px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-37dadf0d192d376510a1\">{\"x\":{\"diagram\":\"digraph {\\ngraph [layout = dot, rankdir = LR]\\n\\n# define the global styles of the nodes. We can override these in box if we wish\\nnode [shape = rectangle, style = filled, color=White, fillcolor = White, fontsize = 40]\\nnodesep=0.02;\\n\\na [label = \\\"O\\\"]\\nfixation [label =  \\\"\\\", shape=circle, fillcolor=Black, width=.2, height=.2]\\nb [label = \\\" \\\"]\\nc [label = \\\" \\\"]\\nd [label = \\\"J\\\"]\\ne [label = \\\" \\\"]\\ne2 [label = \\\" \\\"]\\ne3 [label = \\\"S\\\"]\\n\\n#add some blanks to help space the actual letters together\\nffff [label=\\\"\\\"]\\nfff [label=\\\"\\\"]\\nff [label=\\\"\\\"]\\nf [label = \\\"\\\"]\\ng [label = \\\"R\\\"]\\nh [label = \\\"L\\\"]\\ni [label = \\\"H\\\"]\\nj [label = \\\"Y\\\"]\\nk [label = \\\"M\\\"]\\nl [label = \\\"\\\"]\\nll [label=\\\"\\\"]\\nlll [label=\\\"\\\"]\\nllll [label=\\\"\\\"]\\n\\n# edge definitions with the node IDs\\nedge [label=\\\"\\\", penwidth=0, arrowsize=0]\\na  -> b;\\nedge [label=\\\"\\\", penwidth=0, arrowsize=0]\\nb -> c\\nedge [label=\\\"\\\", penwidth=0, arrowsize=0]\\nc -> d\\nedge [label=\\\"\\\", penwidth=0, arrowsize=0]\\nd -> e\\nedge [label=\\\"\\\", penwidth=0, arrowsize=0]\\ne -> e2\\nedge [label=\\\"\\\", penwidth=0, arrowsize=0]\\ne2 -> e3\\nedge [label=\\\"\\\", penwidth=0, arrowsize=0]\\ne3 -> fixation\\n\\nedge [label=\\\"\\\", penwidth=0, arrowsize=0]\\nfixation -> ffff\\nedge [label=\\\"\\\", penwidth=0, arrowsize=0]\\nffff -> fff\\nedge [label=\\\"\\\", penwidth=0, arrowsize=0]\\nfff -> ff\\nedge [label=\\\"\\\", penwidth=0, arrowsize=0]\\nff -> f\\nedge [label=\\\"\\\", penwidth=0, arrowsize=0]\\nf -> g\\nedge [label=\\\"\\\", penwidth=0, arrowsize=0]\\ng -> h\\nedge [label=\\\"\\\", penwidth=0, arrowsize=0]\\nh -> i\\nedge [label=\\\"\\\", penwidth=0, arrowsize=0]\\ni -> j\\nedge [label=\\\"\\\", penwidth=0, arrowsize=0]\\nj -> k\\nedge [label=\\\"\\\", penwidth=0, arrowsize=0]\\nk -> l\\nedge [label=\\\"\\\", penwidth=0, arrowsize=0]\\nl -> ll\\nedge [label=\\\"\\\", penwidth=0, arrowsize=0]\\nll -> lll\\nedge [label=\\\"\\\", penwidth=0, arrowsize=0]\\nlll -> llll\\n}\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n:::\n:::\n\n\nSpatial\n\n- Not worse with more targets (Holcombe, Chen, & Howe, 2014)\n\n## 5.  \\ \\ \\ Crowding constrains selection: Temporal\n\n\n![](images/2targets3objectsPerArray.gif)\n<!-- https://trackinglimits.whatanimalssee.com/speedAndTime.html#temporal-limits-on-tracking-->\n\n## 5.  \\ \\ \\ Crowding constrains selection: Temporal\n\n![](images/2targets9objectsPerArray.gif)\n<!-- https://trackinglimits.whatanimalssee.com/speedAndTime.html#temporal-limits-on-tracking-->\n\n\n## \n\n![](images/temporalAndSpeedLimitsMsec.png)\n## Wut - it's bizarre I need this to  make the next slide not disappear\n\n## 5.  \\ \\ \\ Crowding constrains selection: Temporal {.smaller}\n\n![](images/temporalAndSpeedLimitsMsec.png)\n\n- Much worse with more targets (Holcombe & Chen, 2013)\n- Hard to explain without serial switching - oscillations\n- Temporal interference not understood in other paradigms (Tkacz-Domb & Yeshurun, 2021)\n\n<!-- - Temporal interference (Verstraten, Cavanagh, & LaBianca, 2000) -->\n\n\n## Five lessons from studies of multiple object tracking\n\n::: {.nonincremental}\n\n1. Feature binding requires focused attention\n2. Where, but not what!\n3. Split-brain selection\n4. Unitary cognition contaminates many studies\n5. Spatial and temporal crowding constrains selection\n\n:::\n\n::: notes\nWith tracking you can actually isolate a hemisphere-specific process.\n\n:::\n\n<!--\n1.  Knowing where but not what. Selecting an object does not entail knowing anything about it, apart from its location\n    -   And it's actually worse than this because\n    -   Don't know about motion direction possibly without focused attention ()\n2.  Location selection and updating is split-brain\n    -   Don't know much about generalization to other tasks\n3.  Feature binding requires focused attention\n    -   Tracking experiments that show you can't use conjunctions\n    -   Including spatial relations Holcombe, Linares, Vaziri-Pashkam\n4.  Different forms of attention work together\n    -   Feature attention exists but is independent of attention\n5.  Unitary (not split-brain) cognition contaminates tasks\n\n-   A unitary (not hemisphere-specific) resource can also contribute to object selection, which can interfere with researcher efforts to study capacity limits.\n\n6.  Spatial and temporal crowding constrain selection\n\n-   only temporal crowding is markedly worsened as the number of objects to select increases.\\\n-   Temporal crowding screws up feature binding\n-   Surface segregation is an exception\n-->\n\n## WikiJournal of Science\n\n![](images/WikiJSci.jpeg)\n\nOpen access • Publication charge free • Public peer review • Wikipedia-integrated\n\n<!--https://en.wikiversity.org/wiki/WikiJournal_Preprints/Multiple_object_tracking-->\n\n::: notes\nI'm an associate editor there\n:::\n\n## WikiJournal Preprints {background-color=\"black\" background-image=\"images/2targets9objectsPerArray.gif\"}\n\n![](images/wikipreprint.png)\n\nhttps://en.wikiversity.org/wiki/WikiJournal_Preprints/Multiple_object_tracking\n\n## END {background-color=\"black\" background-image=\"images/2targets3objectsPerArray.gif\"}\n\n#### Alex Holcombe\n\n#### University of Sydney\n\n## The relationship of multiple object tracking to other forms of attention\n\n-   Feature attention is global\n-   Tracking is hemifield-specific\n\nRemaining questions:\n\n-   Is temporal resolution hemifield-specific?\n-   What about statistical perception?\n-   If you limit time of attention availability with my circular displays, does this prevent doing other tasks (dual tasks)?\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/htmlwidgets-1.6.1/htmlwidgets.js\"></script>\n<script src=\"site_libs/viz-1.8.2/viz.js\"></script>\n<link href=\"site_libs/DiagrammeR-styles-0.2/styles.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/grViz-binding-1.0.9/grViz.js\"></script>\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}